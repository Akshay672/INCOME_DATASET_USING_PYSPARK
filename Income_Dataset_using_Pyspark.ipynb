{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "3aa0b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "23fe0db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "40604a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset link : https://www.kaggle.com/vardhansiramdasu/income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "a7b44c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating basic Spark Session\n",
    "spark=SparkSession.builder.appName('Income_Dataset').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "74d6b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+--------------+-------------+--------------+------+-------+-----------+-----------+------------+--------------+--------------------+\n",
      "|age|     JobType|  EdType| maritalstatus|   occupation|  relationship|  race| gender|capitalgain|capitalloss|hoursperweek| nativecountry|             SalStat|\n",
      "+---+------------+--------+--------------+-------------+--------------+------+-------+-----------+-----------+------------+--------------+--------------------+\n",
      "| 45|     Private| HS-grad|      Divorced| Adm-clerical| Not-in-family| White| Female|          0|          0|          28| United-States| less than or equ...|\n",
      "| 24| Federal-gov| HS-grad| Never-married| Armed-Forces|     Own-child| White|   Male|          0|          0|          40| United-States| less than or equ...|\n",
      "+---+------------+--------+--------------+-------------+--------------+------+-------+-----------+-----------+------------+--------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv('D://M. Tech in Data Science & Machine Learning//Big Data Analytics//Sem_Prep//Income_dataset//Income.csv',header=True,inferSchema=True)\n",
    "df.show(2)#first 2 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "47e820b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'JobType',\n",
       " 'EdType',\n",
       " 'maritalstatus',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'gender',\n",
       " 'capitalgain',\n",
       " 'capitalloss',\n",
       " 'hoursperweek',\n",
       " 'nativecountry',\n",
       " 'SalStat']"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns#list of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "3cf23aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data: ( 13 , 31978 )\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the data: (', len(df.columns),',',df.count(),')')#total records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "37ed5bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- JobType: string (nullable = true)\n",
      " |-- EdType: string (nullable = true)\n",
      " |-- maritalstatus: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capitalgain: integer (nullable = true)\n",
      " |-- capitalloss: integer (nullable = true)\n",
      " |-- hoursperweek: integer (nullable = true)\n",
      " |-- nativecountry: string (nullable = true)\n",
      " |-- SalStat: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()#columns Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "8f7ebb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name:  age  |  column datatype:  int\n",
      "column name:  JobType  |  column datatype:  string\n",
      "column name:  EdType  |  column datatype:  string\n",
      "column name:  maritalstatus  |  column datatype:  string\n",
      "column name:  occupation  |  column datatype:  string\n",
      "column name:  relationship  |  column datatype:  string\n",
      "column name:  race  |  column datatype:  string\n",
      "column name:  gender  |  column datatype:  string\n",
      "column name:  capitalgain  |  column datatype:  int\n",
      "column name:  capitalloss  |  column datatype:  int\n",
      "column name:  hoursperweek  |  column datatype:  int\n",
      "column name:  nativecountry  |  column datatype:  string\n",
      "column name:  SalStat  |  column datatype:  string\n"
     ]
    }
   ],
   "source": [
    "for i , t in df.dtypes:#columns and its datatypes\n",
    "    print('column name: ',i,\" | \", 'column datatype: ',t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "0cd93120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, col, count, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "818173bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-------------+----------+------------+----+------+-----------+-----------+------------+-------------+-------+\n",
      "|age|JobType|EdType|maritalstatus|occupation|relationship|race|gender|capitalgain|capitalloss|hoursperweek|nativecountry|SalStat|\n",
      "+---+-------+------+-------------+----------+------------+----+------+-----------+-----------+------------+-------------+-------+\n",
      "|  0|      0|     0|            0|         0|           0|   0|     0|          0|          0|           0|            0|      0|\n",
      "+---+-------+------+-------------+----------+------------+----+------+-----------+-----------+------------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking for null values\n",
    "df.select([count(when(isnan(c) | col(c).isNull(),c)).alias(c)  for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "a83615f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are no null values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "a85cc987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bucketizer\n",
    "\n",
    "#We now group the based on their age group. \n",
    "#Here, we will use the Bucketizer transformer. \n",
    "#Bucketizer is used for creating group of values of a continuous feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "9208f4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 17|  393|\n",
      "| 18|  542|\n",
      "| 19|  707|\n",
      "| 20|  743|\n",
      "| 21|  709|\n",
      "| 22|  753|\n",
      "| 23|  871|\n",
      "| 24|  784|\n",
      "| 25|  830|\n",
      "| 26|  767|\n",
      "| 27|  820|\n",
      "| 28|  848|\n",
      "| 29|  801|\n",
      "| 30|  842|\n",
      "| 31|  870|\n",
      "| 32|  811|\n",
      "| 33|  862|\n",
      "| 34|  862|\n",
      "| 35|  858|\n",
      "| 36|  875|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('age').count().orderBy('age',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "f87b0b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|min(age)|\n",
      "+--------+\n",
      "|      17|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'age':'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "1d48ae70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|      90|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'age':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "56b02fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can see age groups are in the range of 17 to 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "5d0b450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "dc649d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define the age age group splits\n",
    "splits = [17, 30, 50, 70, 90]\n",
    "bucketizer=Bucketizer( splits=splits,inputCol='age',outputCol='age_group')\n",
    "df1=bucketizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "fbe3d444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|age_group|\n",
      "+---------+\n",
      "|      1.0|\n",
      "|      0.0|\n",
      "|      1.0|\n",
      "|      0.0|\n",
      "|      0.0|\n",
      "|      1.0|\n",
      "|      2.0|\n",
      "|      0.0|\n",
      "|      0.0|\n",
      "|      0.0|\n",
      "|      1.0|\n",
      "|      2.0|\n",
      "|      1.0|\n",
      "|      1.0|\n",
      "|      1.0|\n",
      "|      1.0|\n",
      "|      0.0|\n",
      "|      1.0|\n",
      "|      1.0|\n",
      "|      2.0|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select('age_group').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "6835d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|age_group|count|\n",
      "+---------+-----+\n",
      "|      0.0| 9568|\n",
      "|      1.0|15460|\n",
      "|      2.0| 6329|\n",
      "|      3.0|  621|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupby('age_group').count().orderBy('age_group',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "33eacc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#StringIndexer\n",
    "\n",
    "#There are three categorical variables in our dataset viz., 'JobType', 'EdType', 'maritalstatus', 'occupation', 'relationship', 'race', 'gender', 'nativecountry'. \n",
    "#These variables cannot be directly passed to our ML algorithms. \n",
    "#We will converet them into indexes and to do that we will use StringIndexer transformer. \n",
    "#StringIndexer converts a string column to an index column. The most frequent label gets index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "9e603b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name:  JobType  |  column datatype:  string\n",
      "column name:  EdType  |  column datatype:  string\n",
      "column name:  maritalstatus  |  column datatype:  string\n",
      "column name:  occupation  |  column datatype:  string\n",
      "column name:  relationship  |  column datatype:  string\n",
      "column name:  race  |  column datatype:  string\n",
      "column name:  gender  |  column datatype:  string\n",
      "column name:  nativecountry  |  column datatype:  string\n",
      "column name:  SalStat  |  column datatype:  string\n",
      " \n",
      "['JobType', 'EdType', 'maritalstatus', 'occupation', 'relationship', 'race', 'gender', 'nativecountry', 'SalStat']\n"
     ]
    }
   ],
   "source": [
    "list_categorical_columns=[]\n",
    "for i , t in df1.dtypes:#getting all the categorical variables in our dataset\n",
    "    if t == 'string':\n",
    "        print('column name: ',i,\" | \", 'column datatype: ',t)\n",
    "        list_categorical_columns.append(i)\n",
    "print(' ')\n",
    "print(list_categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "c6801808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "d817eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#also reanme the target variable(SalStat) to 'label'\n",
    "indexer = StringIndexer(inputCols=['JobType', 'EdType', 'maritalstatus', 'occupation', 'relationship', 'race', 'gender', 'nativecountry','SalStat'], \n",
    "                      outputCols=['JobType_index', 'EdType_index', 'maritalstatus_index', 'occupation_index', 'relationship_index', 'race_index', 'gender_index', 'nativecountry_index','label'])\n",
    "df2=indexer.fit(df1).transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "2ada3df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+------------+---------+-------------------+------------+-----+------------------+----------------+-------------+------------+-------------------+----------+\n",
      "|age|capitalgain|capitalloss|hoursperweek|age_group|nativecountry_index|EdType_index|label|relationship_index|occupation_index|JobType_index|gender_index|maritalstatus_index|race_index|\n",
      "+---+-----------+-----------+------------+---------+-------------------+------------+-----+------------------+----------------+-------------+------------+-------------------+----------+\n",
      "|45 |0          |0          |28          |1.0      |0.0                |0.0         |0.0  |1.0               |3.0             |0.0          |1.0         |2.0                |0.0       |\n",
      "|24 |0          |0          |40          |0.0      |0.0                |0.0         |0.0  |2.0               |14.0            |6.0          |0.0         |1.0                |0.0       |\n",
      "+---+-----------+-----------+------------+---------+-------------------+------------+-----+------------------+----------------+-------------+------------+-------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df2.drop('JobType', 'EdType', 'maritalstatus', 'occupation', 'relationship', 'race', 'gender', 'nativecountry','SalStat')\n",
    "df2.show(2,False)#dropped unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "83c594cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VectorAssembler\n",
    "\n",
    "#MLlib expects all features to be contained within a single column. \n",
    "#VectorAssembler combines multiple columns and gives single column as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "570c4c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "8571c5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'capitalgain',\n",
       " 'capitalloss',\n",
       " 'hoursperweek',\n",
       " 'age_group',\n",
       " 'nativecountry_index',\n",
       " 'EdType_index',\n",
       " 'label',\n",
       " 'relationship_index',\n",
       " 'occupation_index',\n",
       " 'JobType_index',\n",
       " 'gender_index',\n",
       " 'maritalstatus_index',\n",
       " 'race_index']"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "e0d85c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols=['age','capitalgain','capitalloss','hoursperweek','age_group','nativecountry_index','EdType_index','relationship_index','occupation_index','JobType_index','gender_index','maritalstatus_index','race_index']\n",
    "assembler=VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df3=assembler.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "fd0f8eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+------------+---------+-------------------+------------+-----+------------------+----------------+-------------+------------+-------------------+----------+------------------------------------------------------+\n",
      "|age|capitalgain|capitalloss|hoursperweek|age_group|nativecountry_index|EdType_index|label|relationship_index|occupation_index|JobType_index|gender_index|maritalstatus_index|race_index|features                                              |\n",
      "+---+-----------+-----------+------------+---------+-------------------+------------+-----+------------------+----------------+-------------+------------+-------------------+----------+------------------------------------------------------+\n",
      "|45 |0          |0          |28          |1.0      |0.0                |0.0         |0.0  |1.0               |3.0             |0.0          |1.0         |2.0                |0.0       |(13,[0,3,4,7,8,10,11],[45.0,28.0,1.0,1.0,3.0,1.0,2.0])|\n",
      "|24 |0          |0          |40          |0.0      |0.0                |0.0         |0.0  |2.0               |14.0            |6.0          |0.0         |1.0                |0.0       |(13,[0,3,7,8,9,11],[24.0,40.0,2.0,14.0,6.0,1.0])      |\n",
      "+---+-----------+-----------+------------+---------+-------------------+------------+-----+------------------+----------------+-------------+------------+-------------------+----------+------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "8c6912da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+-----+\n",
      "|features                                              |label|\n",
      "+------------------------------------------------------+-----+\n",
      "|(13,[0,3,4,7,8,10,11],[45.0,28.0,1.0,1.0,3.0,1.0,2.0])|0.0  |\n",
      "|(13,[0,3,7,8,9,11],[24.0,40.0,2.0,14.0,6.0,1.0])      |0.0  |\n",
      "+------------------------------------------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select('features','label').show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "5441c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VectorIndexer\n",
    "\n",
    "#VectorIndexer automatically identifies the categorical features from the feature vector (output from VectorAssembler). \n",
    "#It then indexes categorical features inside of a Vector It is the vectorized version of StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "fc717886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "ba704172",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_indexer=VectorIndexer(inputCol='features', outputCol=\"indexed_features\")\n",
    "df4=vector_indexer.fit(df3).transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "0f95b83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+------------+---------+-------------------+------------+-----+------------------+----------------+-------------+------------+-------------------+----------+------------------------------------------------------+------------------------------------------------------+\n",
      "|age|capitalgain|capitalloss|hoursperweek|age_group|nativecountry_index|EdType_index|label|relationship_index|occupation_index|JobType_index|gender_index|maritalstatus_index|race_index|features                                              |indexed_features                                      |\n",
      "+---+-----------+-----------+------------+---------+-------------------+------------+-----+------------------+----------------+-------------+------------+-------------------+----------+------------------------------------------------------+------------------------------------------------------+\n",
      "|45 |0          |0          |28          |1.0      |0.0                |0.0         |0.0  |1.0               |3.0             |0.0          |1.0         |2.0                |0.0       |(13,[0,3,4,7,8,10,11],[45.0,28.0,1.0,1.0,3.0,1.0,2.0])|(13,[0,3,4,7,8,10,11],[45.0,28.0,1.0,1.0,3.0,1.0,2.0])|\n",
      "|24 |0          |0          |40          |0.0      |0.0                |0.0         |0.0  |2.0               |14.0            |6.0          |0.0         |1.0                |0.0       |(13,[0,3,7,8,9,11],[24.0,40.0,2.0,14.0,6.0,1.0])      |(13,[0,3,7,8,9,11],[24.0,40.0,2.0,14.0,6.0,1.0])      |\n",
      "+---+-----------+-----------+------------+---------+-------------------+------------+-----+------------------+----------------+-------------+------------+-------------------+----------+------------------------------------------------------+------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "fcec65af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+------------------------------------------------------+-----+\n",
      "|features                                              |indexed_features                                      |label|\n",
      "+------------------------------------------------------+------------------------------------------------------+-----+\n",
      "|(13,[0,3,4,7,8,10,11],[45.0,28.0,1.0,1.0,3.0,1.0,2.0])|(13,[0,3,4,7,8,10,11],[45.0,28.0,1.0,1.0,3.0,1.0,2.0])|0.0  |\n",
      "|(13,[0,3,7,8,9,11],[24.0,40.0,2.0,14.0,6.0,1.0])      |(13,[0,3,7,8,9,11],[24.0,40.0,2.0,14.0,6.0,1.0])      |0.0  |\n",
      "+------------------------------------------------------+------------------------------------------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.select('features','indexed_features','label').show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "e9c9f806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-Test Split\n",
    "\n",
    "#We split the output of data into training and test sets (30% held out for testing) \n",
    "#Note: This train-test split of for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "07737fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = df4.randomSplit([0.7,0.3],seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "c6fc050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train1.count(),test1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "a4fc364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Supervised Learning - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "c6cbdab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "53576add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "39127e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression(featuresCol='features',labelCol='label')\n",
    "lrmodel=lr.fit(train1)\n",
    "#lr_prediction=lrmodel.transform(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "3ce8b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "8ecbb31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.815756602426838\n",
      "Area under the ROC curve =  0.8564887315363008\n",
      "Precision =  0.80201013241653\n",
      "Recall =  0.815756602426838\n",
      "F1 Score =  0.799329362511479\n"
     ]
    }
   ],
   "source": [
    "# Create model summary object\n",
    "lrmodelSummary = lrmodel.summary\n",
    "\n",
    "# Print the following metrics one by one: \n",
    "# 1. Accuracy\n",
    "# Accuracy is a model summary parameter\n",
    "print(\"Accuracy = \", lrmodelSummary.accuracy)\n",
    "# 2. Area under the ROC curve\n",
    "# Area under the ROC curve is a model summary parameter\n",
    "print(\"Area under the ROC curve = \", lrmodelSummary.areaUnderROC)\n",
    "# 3. Precision (Positive Predictive Value)\n",
    "# Precision is a model summary parameter\n",
    "print(\"Precision = \", lrmodelSummary.weightedPrecision)\n",
    "# 4. Recall (True Positive Rate)\n",
    "# Recall is a model summary parameter\n",
    "print(\"Recall = \", lrmodelSummary.weightedRecall)\n",
    "# 5. F1 Score (F-measure)\n",
    "# F1 Score is a model summary method\n",
    "print(\"F1 Score = \", lrmodelSummary.weightedFMeasure())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "e1e73884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "33d17759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "8b1febcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=DecisionTreeClassifier(featuresCol='indexed_features',labelCol='label',maxDepth= 10,maxBins=41)\n",
    "dt_model=dt.fit(train1)\n",
    "dt_prediction=dt_model.transform(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "0e706192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'capitalgain',\n",
       " 'capitalloss',\n",
       " 'hoursperweek',\n",
       " 'age_group',\n",
       " 'nativecountry_index',\n",
       " 'EdType_index',\n",
       " 'label',\n",
       " 'relationship_index',\n",
       " 'occupation_index',\n",
       " 'JobType_index',\n",
       " 'gender_index',\n",
       " 'maritalstatus_index',\n",
       " 'race_index',\n",
       " 'features',\n",
       " 'indexed_features',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_prediction.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "b46b022c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------------------------+----------+-----+\n",
      "|rawPrediction|probability                               |prediction|label|\n",
      "+-------------+------------------------------------------+----------+-----+\n",
      "|[3600.0,1.0] |[0.9997222993612885,2.7770063871146905E-4]|0.0       |0.0  |\n",
      "|[3600.0,1.0] |[0.9997222993612885,2.7770063871146905E-4]|0.0       |0.0  |\n",
      "|[280.0,1.0]  |[0.99644128113879,0.0035587188612099642]  |0.0       |0.0  |\n",
      "|[3600.0,1.0] |[0.9997222993612885,2.7770063871146905E-4]|0.0       |0.0  |\n",
      "|[3600.0,1.0] |[0.9997222993612885,2.7770063871146905E-4]|0.0       |0.0  |\n",
      "|[3600.0,1.0] |[0.9997222993612885,2.7770063871146905E-4]|0.0       |0.0  |\n",
      "|[3600.0,1.0] |[0.9997222993612885,2.7770063871146905E-4]|0.0       |0.0  |\n",
      "|[3600.0,1.0] |[0.9997222993612885,2.7770063871146905E-4]|0.0       |0.0  |\n",
      "|[3600.0,1.0] |[0.9997222993612885,2.7770063871146905E-4]|0.0       |0.0  |\n",
      "|[3600.0,1.0] |[0.9997222993612885,2.7770063871146905E-4]|0.0       |0.0  |\n",
      "+-------------+------------------------------------------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_prediction.select('rawPrediction','probability','prediction','label').show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "c9c30e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "a8c0cab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8432048897259496\n",
      "Area under the ROC curve =  0.7864135261704837\n",
      "Precision =  0.8432048897259496\n",
      "Recall =  0.8432048897259496\n",
      "F1 Score =  0.8432048897259496\n"
     ]
    }
   ],
   "source": [
    "# import MulticlassClassificationEvaluator from the pyspark.ml.evaluation package\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Build the MulticlassClassificationEvaluator object 'evaluator'\n",
    "multievaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# 1. Accuracy\n",
    "print(\"Accuracy: \", multievaluator.evaluate(dt_prediction, {evaluator.metricName: \"accuracy\"})) \n",
    "# 2. Area under the ROC curve\n",
    "print('Area under the ROC curve = ', evaluator.evaluate(dt_prediction))\n",
    "# 3. Precision (Positive Predictive Value)\n",
    "print(\"Precision = \", multievaluator.evaluate(dt_prediction, {evaluator.metricName: \"weightedPrecision\"}))\n",
    "# 4. Recall (True Positive Rate)\n",
    "print(\"Recall = \", multievaluator.evaluate(dt_prediction, {evaluator.metricName: \"weightedRecall\"}))\n",
    "# 5. F1 Score (F-measure)\n",
    "print(\"F1 Score = \", multievaluator.evaluate(dt_prediction, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "010e5310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "fe63128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "2b5460c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(featuresCol='indexed_features',labelCol='label',maxBins=41)\n",
    "rf_model=rf.fit(train1)\n",
    "rf_prediction=rf_model.transform(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "3d0dbd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'capitalgain',\n",
       " 'capitalloss',\n",
       " 'hoursperweek',\n",
       " 'age_group',\n",
       " 'nativecountry_index',\n",
       " 'EdType_index',\n",
       " 'label',\n",
       " 'relationship_index',\n",
       " 'occupation_index',\n",
       " 'JobType_index',\n",
       " 'gender_index',\n",
       " 'maritalstatus_index',\n",
       " 'race_index',\n",
       " 'features',\n",
       " 'indexed_features',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_prediction.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "416e363e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+-----------------------------------------+----------+-----+\n",
      "|rawPrediction                          |probability                              |prediction|label|\n",
      "+---------------------------------------+-----------------------------------------+----------+-----+\n",
      "|[19.249040465247806,0.750959534752191] |[0.9624520232623904,0.03754797673760956] |0.0       |0.0  |\n",
      "|[19.249040465247806,0.750959534752191] |[0.9624520232623904,0.03754797673760956] |0.0       |0.0  |\n",
      "|[18.407606713228706,1.5923932867712878]|[0.9203803356614356,0.07961966433856442] |0.0       |0.0  |\n",
      "|[19.181641709440473,0.8183582905595226]|[0.9590820854720239,0.040917914527976135]|0.0       |0.0  |\n",
      "|[19.249040465247806,0.750959534752191] |[0.9624520232623904,0.03754797673760956] |0.0       |0.0  |\n",
      "|[19.249040465247806,0.750959534752191] |[0.9624520232623904,0.03754797673760956] |0.0       |0.0  |\n",
      "|[19.1686714716429,0.8313285283570966]  |[0.9584335735821451,0.04156642641785484] |0.0       |0.0  |\n",
      "|[19.249040465247806,0.750959534752191] |[0.9624520232623904,0.03754797673760956] |0.0       |0.0  |\n",
      "|[19.1686714716429,0.8313285283570966]  |[0.9584335735821451,0.04156642641785484] |0.0       |0.0  |\n",
      "|[19.1686714716429,0.8313285283570966]  |[0.9584335735821451,0.04156642641785484] |0.0       |0.0  |\n",
      "+---------------------------------------+-----------------------------------------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_prediction.select('rawPrediction','probability','prediction','label').show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "86a961fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "multievaluator2=MulticlassClassificationEvaluator(predictionCol='prediction',labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "f6834d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.83113116405949\n",
      "Area under the ROC curve =  0.8951071472522564\n",
      "Precision =  0.83113116405949\n",
      "Recall =  0.83113116405949\n",
      "F1 Score =  0.83113116405949\n"
     ]
    }
   ],
   "source": [
    "# 1. Accuracy\n",
    "print(\"Accuracy: \", multievaluator.evaluate(rf_prediction, {evaluator.metricName: \"accuracy\"})) \n",
    "# 2. Area under the ROC curve\n",
    "print('Area under the ROC curve = ', evaluator.evaluate(rf_prediction))\n",
    "# 3. Precision (Positive Predictive Value)\n",
    "print(\"Precision = \", multievaluator.evaluate(rf_prediction, {evaluator.metricName: \"weightedPrecision\"}))\n",
    "# 4. Recall (True Positive Rate)\n",
    "print(\"Recall = \", multievaluator.evaluate(rf_prediction, {evaluator.metricName: \"weightedRecall\"}))\n",
    "# 5. F1 Score (F-measure)\n",
    "print(\"F1 Score = \", multievaluator.evaluate(rf_prediction, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "4f87d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------END---------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "9f60f441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations in training set =  22416\n",
      "Observations in testing set =  9562\n"
     ]
    }
   ],
   "source": [
    "## We spilt the data into 70-30 set\n",
    "# Training Set - 70% obesevations\n",
    "# Testing Set - 30% observations\n",
    "trainDF, testDF =  df.randomSplit([0.7,0.3], seed = 2)\n",
    "\n",
    "# print the count of observations in each set\n",
    "print(\"Observations in training set = \", trainDF.count())\n",
    "print(\"Observations in testing set = \", testDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "4d97c71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+\n",
      "|label|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|  0.0|[4.39909452909074...|[0.98786071147062...|       0.0|\n",
      "|  0.0|[3.80032493739012...|[0.97812568244983...|       0.0|\n",
      "|  0.0|[-7.9715523917419...|[3.45023690571653...|       1.0|\n",
      "|  0.0|[3.48951829248258...|[0.97038805705483...|       0.0|\n",
      "|  0.0|[3.11995804025573...|[0.95770852869916...|       0.0|\n",
      "|  0.0|[2.75039778802888...|[0.93993581146911...|       0.0|\n",
      "|  0.0|[2.75039778802888...|[0.93993581146911...|       0.0|\n",
      "|  0.0|[3.50895248600611...|[0.97094142399573...|       0.0|\n",
      "|  0.0|[3.87189297726296...|[0.97960566570795...|       0.0|\n",
      "|  0.0|[3.13277247280925...|[0.95822451694094...|       0.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import Pipeline from pyspark.ml package\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Build the pipeline object by providing stages(transformers + Estimator) \n",
    "# that you need the dataframe to pass through\n",
    "# Transfoermers - binarizer, bucketizer, indexers, encoder, assembler\n",
    "# Estimator - lr\n",
    "lrpipeline = Pipeline(stages=[bucketizer, indexer, assembler, lr])\n",
    "\n",
    "# fit the pipeline for the trainind data\n",
    "lrpipelinemodel = lrpipeline.fit(trainDF)\n",
    "\n",
    "# transform the data\n",
    "lrpipelinepredicted = lrpipelinemodel.transform(testDF)\n",
    "\n",
    "# view some of the columns generated\n",
    "lrpipelinepredicted.select('label', 'rawPrediction', 'probability', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "b3586154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8002775080005335\n",
      "Area under the ROC curve =  0.8584433805887849\n",
      "Precision =  0.8002775080005335\n",
      "Recall =  0.8002775080005335\n",
      "F1 Score =  0.8002775080005335\n"
     ]
    }
   ],
   "source": [
    "# Build the MulticlassClassificationEvaluator object 'evaluator'\n",
    "multievaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# 1. Accuracy\n",
    "print(\"Accuracy: \", multievaluator.evaluate(lrpipelinepredicted, {evaluator.metricName: \"accuracy\"})) \n",
    "# 2. Area under the ROC curve\n",
    "print('Area under the ROC curve = ', evaluator.evaluate(lrpipelinepredicted))\n",
    "# 3. Precision (Positive Predictive Value)\n",
    "print(\"Precision = \", multievaluator.evaluate(lrpipelinepredicted, {evaluator.metricName: \"weightedPrecision\"}))\n",
    "# 4. Recall (True Positive Rate)\n",
    "print(\"Recall = \", multievaluator.evaluate(lrpipelinepredicted, {evaluator.metricName: \"weightedRecall\"}))\n",
    "# 5. F1 Score (F-measure)\n",
    "print(\"F1 Score = \", multievaluator.evaluate(lrpipelinepredicted, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "4d9106da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------end-----------------------------------------------------#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
